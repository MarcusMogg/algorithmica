
许多计算机科学的书籍都会在一开始引入**算法复杂度** 的概念，简单来说，就是计算过程中所有基本操作（加、减、乘、除……）的总和，有的时候也会按操作的成本进行加权。

算法复杂度是一个古老的概念，在1960年代早期系统制定，在那之后广泛应用于算法设计的代价函数。这个模型被广泛采用的原因是它很好的近似了当时计算机的工作方式。
## 经典复杂度理论

CPU的基本操作被称为**指令（instructions）**，它们的成本被称为**延迟（latencies）**。 指令被存储在内存中，并被处理器一条一条的处理，处理器有一系列**寄存器（register）** 存储内部状态。其中一个存储器被称为指令指针（instruction pointer，IP），用于指示下一条需要读取和执行的指令。每条指令会改变处理器的状态，可能会修改主存，，并且需要不同的CPU周期来完成，然后才能启动下一条指令。


为了估计程序的真实执行时间，你需要计算它所有执行指令的延迟的总和，并除以 时钟频率，即一个特定型号CPU每秒执行的周期数。

![](img/cpu.png)


时钟频率是一个不稳定且通常未知的变量，取决于 CPU 型号、操作系统设置、当前芯片温度、其他组件的功耗以及许多其他因素。相比之下，指令延时是固定的，用时钟周期表示，在不同的CPU之间甚至有些一致，因此对它们进行计数对于分析目的更有用。

例如，按定义矩阵乘法算法需要  $n^2 \cdot (n + n - 1)$ 次算术运算：具体来说，$n^3$ 次乘法和  $n^2 \cdot (n - 1)$ 次加法。如果我们查找这些指令的延迟（在称为指令表的特殊文档中，例如 [this one](https://www.agner.org/optimize/instruction_tables.pdf)），我们可以发现，乘法需要 3 个周期，而加法需要 1 个周期，因此整个计算需要$3 \cdot n^3 + n^2 \cdot (n - 1) = 4 \cdot n^3 - n^2$ 时钟周期。

类似于 将指令延迟的总和用作总执行时间，计算复杂度可用于量化抽象算法的内在时间要求，而无需依赖特定计算机的选择。

## 渐进复杂度

将执行时间表示为输入大小的函数现在看起来很显然，但在1960年代并非如此。那时候的计算机又大又贵，执行速度还慢。多被用于实际任务，比如预测天气，将火箭送入太空，或者弄清楚苏联核导弹可以从古巴海岸飞多远——所有这些都是有限长度的问题。那个时代的工程师主要关心如何计算 $3 \times 3$ 矩阵而不是 $n \times n$ 矩阵。

导致这种转变的原因是计算机科学家相信 计算机将变得更快——事实上确实如此。随着时间的推移，人们停止计算执行时间，然后停止计算周期，甚至停止完全计数操作，取而代之的是估计值，在足够大的输入上，只差不超过一个常数因子。随着渐近的复杂性，冗长的“ $4 \cdot n^3 - n^2$ 操作”变成了简单的 "$\Theta(n^3)$"，将单个操作的成本以及硬件的所有其他复杂性隐藏在”大O“中 。

![](img/complexity.jpg)

我们使用渐近复杂性的原因是：它足够简单，同时仍然足够精确，可以在大型数据集上产生算法性能的有用结果。在计算机最终将变得足够快以在合理的时间内处理任何足够大的输入的承诺下，渐近复杂度更快的算法 真实跑起来也会更快，无论隐藏常数如何。

但事实证明，这个承诺是不正确的——至少在时钟速度和指令延迟方面不是这样——在本章中，我们将尝试解释为什么以及如何处理它。